{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b1a9e578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "389bcb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X', 'T', 'n', 'u', 'p', 'q', 'E']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(400200,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 指定目标文件夹\n",
    "output_folder = \"data_2_origin/LD\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# 加载 npz 文件\n",
    "data = np.load(\"simulation/data_LD_6_gn_001.npz\")\n",
    "\n",
    "# 列出文件中的键\n",
    "print(data.files)  # 显示 npz 文件中的所有键\n",
    "\n",
    "X = data['X'].flatten()\n",
    "T = data['T'].flatten()\n",
    "n = data['n'].flatten()\n",
    "u = data['u'].flatten()\n",
    "p = data['p'].flatten()\n",
    "q = data['q'].flatten()\n",
    "E = data['E'].flatten()\n",
    "df = pd.DataFrame({\n",
    "    'T': T,\n",
    "    'X': X,\n",
    "    'n': n,\n",
    "    'u': u,\n",
    "    'p': p,\n",
    "    'q': q,\n",
    "    'E': E\n",
    "})\n",
    "output_path = 'data_2_origin/LD/LD_gn_001.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "# 假设数据有多个键，例如 X, T, n, u, p, q, E\n",
    "# 将每个键的数据转换为 csv 文件\n",
    "# for key in data.files:\n",
    "#     np_data = data[key].flatten()\n",
    "#     # 检查是否为二维数组\n",
    "#     if len(np_data.shape) == 2:\n",
    "#         # 保存为 csv 文件\n",
    "#         file_path = os.path.join(output_folder, f\"LD_gn_3_{key}.csv\")\n",
    "#         pd.DataFrame(np_data).to_csv(file_path, index=False, header=False)\n",
    "#     else:\n",
    "#         print(f\"{key} is not a 2D array, skipping...\")\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "67bda42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.autograd import grad\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "9d1067ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerWithPhysics(\n",
       "  (embedding): Linear(in_features=2, out_features=128, bias=True)\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc_out): Linear(in_features=128, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义 TransformerWithPhysics 类\n",
    "class TransformerWithPhysics(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, num_layers, num_heads, dropout=0.1):\n",
    "        super(TransformerWithPhysics, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads, dropout=dropout)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(1, 0, 2)  # Transformer expects (seq_len, batch_size, hidden_dim)\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # Convert back to (batch_size, seq_len, hidden_dim)\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "# 初始化模型\n",
    "input_dim = 2  # 输入维度\n",
    "hidden_dim = 128  # 隐藏层大小\n",
    "output_dim = 5  # 输出维度\n",
    "num_layers = 4  # Transformer层数\n",
    "num_heads = 8  # 注意力头数\n",
    "\n",
    "model = TransformerWithPhysics(input_dim, output_dim, hidden_dim, num_layers, num_heads)\n",
    "\n",
    "# 加载已经训练好的模型参数\n",
    "snapshot = torch.load(\"models/ld_tf_ddp_3/gn/0005/snapshot.pt\")\n",
    "\n",
    "model_state_dict = snapshot[\"model_state_dict\"]\n",
    "if 'module.' in next(iter(model_state_dict.keys())):\n",
    "    model_state_dict = {k.replace('module.', ''): v for k, v in model_state_dict.items()}\n",
    "model.load_state_dict(model_state_dict)\n",
    "# model.load_state_dict(torch.load('models/model_LD_TF/3/transformer_model_epoch_0.pth'))\n",
    "\n",
    "# 将模型移动到 GPU 上（如果有GPU的话）\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else\n",
    "                      'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# 将模型设置为评估模式，这会关闭 dropout 等层\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "5f132d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "\n",
    "# 释放未使用的显存\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 加载数据\n",
    "data = np.load('simulation/data_LD_6.npz')\n",
    "X, T = data[\"X\"].flatten(), data[\"T\"].flatten()\n",
    "Y_true = np.column_stack([(data[key]).flatten() for key in \"nupqE\"])\n",
    "\n",
    "# 将 X 和 T 转换为 3D 形状 (batch_size, seq_len, input_dim)\n",
    "batch_size = 200  # 适当减少批量大小\n",
    "seq_len = len(X) // batch_size\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float32).view(batch_size, seq_len, 1)\n",
    "T = torch.tensor(T, dtype=torch.float32).view(batch_size, seq_len, 1)\n",
    "\n",
    "# 使用模型进行预测\n",
    "Y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(batch_size):\n",
    "        x_tf = torch.cat((T[i:i+1], X[i:i+1]), dim=2).to(device)\n",
    "        y_pred_batch = model(x_tf).cpu().numpy()\n",
    "        Y_pred.append(y_pred_batch)\n",
    "\n",
    "# 将预测结果转换为 NumPy 数组\n",
    "Y_pred = np.concatenate(Y_pred, axis=0)\n",
    "\n",
    "# 打印预测结果\n",
    "\n",
    "Y_pred = Y_pred.reshape(-1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "bf734e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error = abs(Y_true - Y_pred)\n",
    "data = np.load('simulation/data_LD_6.npz')\n",
    "X, T = data[\"X\"].flatten(), data[\"T\"].flatten()\n",
    "# n_ae = Error[:, 0].flatten()\n",
    "# u_ae = Error[:, 1].flatten()\n",
    "# p_ae = Error[:, 2].flatten()\n",
    "# q_ae = Error[:, 3].flatten()\n",
    "# E_ae = Error[:, 4].flatten()\n",
    "n_pred = Y_pred[:, 0].flatten()\n",
    "u_pred = Y_pred[:, 1].flatten()\n",
    "p_pred = Y_pred[:, 2].flatten()\n",
    "q_pred = Y_pred[:, 3].flatten()\n",
    "E_pred = Y_pred[:, 4].flatten()\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'T': T,\n",
    "    'X': X,\n",
    "    'n': n,\n",
    "    'u': u,\n",
    "    'p': p,\n",
    "    'q': q,\n",
    "    'E': E\n",
    "})\n",
    "output_path = 'data_2_origin/LD_pred/LD_gn_0005_pred.csv'\n",
    "df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "e7c65a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.000e+00, 1.000e-02, 2.000e-02, ..., 1.998e+01, 1.999e+01,\n",
       "       2.000e+01])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ind = (T<=10)\n",
    "# Y_true = Y_true[ind]\n",
    "# Y_pred = Y_pred[ind]\n",
    "t, x = T.reshape(2001, 200), X.reshape(2001, 200)\n",
    "dx = x[0, 1] - x[0, 0]\n",
    "t_val = t[:, 0]\n",
    "# ind_t = (t_val<=10)\n",
    "# t_val = t_val[ind]\n",
    "\n",
    "t_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "56aca0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ind = (t_val<=10)\n",
    "# ind\n",
    "# t_val = t_val[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "1603c6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "E_true = Y_true[:,-1].reshape(2001, 200)\n",
    "E_pred = Y_pred[:,-1].reshape(2001, 200)\n",
    "energy_true = (E_true**2).sum(axis=1)*dx\n",
    "energy_pred = (E_pred**2).sum(axis=1)*dx\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'T': t_val,\n",
    "    'Energy_simulation': energy_true,\n",
    "    'Energy_prediction': energy_pred\n",
    "})\n",
    "output_path = 'data_2_origin/LD_pred/LD_gn_0005_rate.csv'\n",
    "\n",
    "df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0606e9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pred_folder = \"data_2_origin/LD_gn_3_pred\"\n",
    "os.makedirs(output_pred_folder, exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "37039610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400200,)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_rs = Y_pred[:, 0].flatten()\n",
    "u_rs = Y_pred[:, 1].flatten()\n",
    "p_rs = Y_pred[:, 2].flatten()\n",
    "q_rs = Y_pred[:, 3].flatten()\n",
    "E_rs = Y_pred[:, 4].flatten()\n",
    "X = data['X'].flatten()\n",
    "T = data['T'].flatten()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d9618eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'T': T,\n",
    "    'X': X,\n",
    "    'n': n_rs,\n",
    "    'u': u_rs,\n",
    "    'p': p_rs,\n",
    "    'q': q_rs,\n",
    "    'E': E_rs\n",
    "})\n",
    "output_path = 'data_2_origin/LD_gn_3_pred/LD_gn_3_pred.csv'\n",
    "df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e7083790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 5)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_rs = np.load('simulation/data_LD_6.npz')\n",
    "X, T = data_rs[\"X\"].flatten(), data_rs[\"T\"].flatten()\n",
    "Y_full = np.column_stack([(data_rs[key]).flatten() for key in \"nupqE\"])\n",
    "\n",
    "#     step = 8\n",
    "#     ind = np.arange(0, X.shape[0], step)\n",
    "ind = np.random.randint(0,X.shape,size=4000)\n",
    "X_train = X[ind]\n",
    "T_train = T[ind]\n",
    "Y_train = Y_full[ind]\n",
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ec220772",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_train = T_train.flatten()\n",
    "X_train = X_train.flatten()\n",
    "p_train = Y_train[:, 2].flatten()\n",
    "q_train = Y_train[:, 3].flatten()\n",
    " \n",
    "df = pd.DataFrame({\n",
    "    'T': T_train,\n",
    "    'X': X_train,\n",
    "    'p': Y_train[:, 2],\n",
    "    'q': Y_train[:, 3]\n",
    "})\n",
    "output_path = 'data_2_origin/LD/LD_rs.csv'\n",
    "\n",
    "df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ff31f4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "t, x = T.reshape(2001, 200), X.reshape(2001, 200)\n",
    "dx = x[0, 1] - x[0, 0]\n",
    "t_val = t[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6ba8fb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "E_true = Y_true[:,-1].reshape(2001, 200)\n",
    "E_pred = Y_pred[:,-1].reshape(2001, 200)\n",
    "energy_true = (E_true**2).sum(axis=1)*dx\n",
    "energy_pred = (E_pred**2).sum(axis=1)*dx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1e99d107",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'T': t_val,\n",
    "    'Energy_simulation': energy_true,\n",
    "    'Energy_prediction': energy_pred\n",
    "})\n",
    "output_path = 'data_2_origin/LD_pred/LD_q_rate.csv'\n",
    "\n",
    "df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf7887ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerWithPhysics(\n",
       "  (embedding): Linear(in_features=2, out_features=128, bias=True)\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc_out): Linear(in_features=128, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义 TransformerWithPhysics 类\n",
    "class TransformerWithPhysics(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, num_layers, num_heads, dropout=0.1):\n",
    "        super(TransformerWithPhysics, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads, dropout=dropout)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(1, 0, 2)  # Transformer expects (seq_len, batch_size, hidden_dim)\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # Convert back to (batch_size, seq_len, hidden_dim)\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "# 初始化模型\n",
    "input_dim = 2  # 输入维度\n",
    "hidden_dim = 128  # 隐藏层大小\n",
    "output_dim = 5  # 输出维度\n",
    "num_layers = 4  # Transformer层数\n",
    "num_heads = 8  # 注意力头数\n",
    "\n",
    "model = TransformerWithPhysics(input_dim, output_dim, hidden_dim, num_layers, num_heads)\n",
    "\n",
    "# 加载已经训练好的模型参数\n",
    "snapshot = torch.load(\"models/ld_tf_ddp_3/gn_3/snapshot.pt\")\n",
    "\n",
    "model_state_dict = snapshot[\"model_state_dict\"]\n",
    "if 'module.' in next(iter(model_state_dict.keys())):\n",
    "    model_state_dict = {k.replace('module.', ''): v for k, v in model_state_dict.items()}\n",
    "model.load_state_dict(model_state_dict)\n",
    "# model.load_state_dict(torch.load('models/model_LD_TF/3/transformer_model_epoch_0.pth'))\n",
    "\n",
    "# 将模型移动到 GPU 上（如果有GPU的话）\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else\n",
    "                      'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# 将模型设置为评估模式，这会关闭 dropout 等层\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b2eff91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "\n",
    "# 释放未使用的显存\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 加载数据\n",
    "data = np.load('simulation/data_LD_6.npz')\n",
    "X, T = data[\"X\"].flatten(), data[\"T\"].flatten()\n",
    "Y_true = np.column_stack([(data[key]).flatten() for key in \"nupqE\"])\n",
    "\n",
    "# 将 X 和 T 转换为 3D 形状 (batch_size, seq_len, input_dim)\n",
    "batch_size = 200  # 适当减少批量大小\n",
    "seq_len = len(X) // batch_size\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float32).view(batch_size, seq_len, 1)\n",
    "T = torch.tensor(T, dtype=torch.float32).view(batch_size, seq_len, 1)\n",
    "\n",
    "# 使用模型进行预测\n",
    "Y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(batch_size):\n",
    "        x_tf = torch.cat((T[i:i+1], X[i:i+1]), dim=2).to(device)\n",
    "        y_pred_batch = model(x_tf).cpu().numpy()\n",
    "        Y_pred.append(y_pred_batch)\n",
    "\n",
    "# 将预测结果转换为 NumPy 数组\n",
    "Y_pred = np.concatenate(Y_pred, axis=0)\n",
    "\n",
    "# 打印预测结果\n",
    "\n",
    "Y_pred = Y_pred.reshape(-1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "94c1d86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_squared(y_true, y_pred):\n",
    "    ss_res = sum((y_true - y_pred) ** 2)  # Residual sum of squares\n",
    "    ss_tot = sum((y_true - np.mean(y_true)) ** 2)  # Total sum of squares\n",
    "    return 1 - (ss_res / ss_tot)  # R-squared calculation\n",
    "\n",
    "R_squared = r_squared(Y_true, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "c6b59509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999233229880389"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_sqr = np.average(R_squared)\n",
    "R_sqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67eb5186",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base]",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
